---
layout: post
title: K8S(04)核心插件-coredns 服务
categories: k8s,docker
description: coredns 在 K8S 中的用途,主要是用作服务发现，也就是服务(应用)之间相互定位的过程。
keywords: k8s,docker
---

> coredns 在 K8S 中的用途,主要是用作服务发现，也就是服务(应用)之间相互定位的过程。

## 1 coredns 用途

[coredns github地址](https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/dns/coredns/coredns.yaml.base)

coredns 都做了什么：[Kubernetes内部域名解析原理、弊端及优化方式](http://ccnuo.com/2019/08/25/CoreDNS：Kubernetes内部域名解析原理、弊端及优化方式/)

### 1.1 为什么需要服务发现

**在 K8S 集群中，Pod 有以下特性：**

1. 服务动态性强
   容器在 k8s 中迁移会导致 Pod 的 IP 地址变化
2. 更新发布频繁
   版本迭代快，新旧 Pod 的 IP 地址会不同
3. 支持自动伸缩
   大促或流量高峰需要动态伸缩，IP 地址会动态增减

**service 资源解决 Pod 服务发现：**

- ①为了解决 Pod 地址变化的问题，需要部署 service 资源
- ②用 service 资源代理后端 Pod
- ③通过暴露 service 资源的固定地址(集群 IP )，来解决以上 Pod 资源变化产生的 IP 变动问题

**那 service 资源的服务发现呢？**

service 资源提供了一个不变的集群 IP 供外部访问，但

1. IP 地址毕竟难以记忆
2. service 资源可能也会被销毁和创建
3. 能不能将 service 资源名称和 service 暴露的集群网络 IP 对于
4. 类似域名与 IP 关系，则只需记服务名就能自动匹配服务 IP
5. 岂不就达到了 service 服务的自动发现

**在 k8s 中，coredns 就是为了解决以上问题。**

## 2 coredns 的部署

从 coredns 开始，我们使用声明式向 k8s 中交付容器的方式，来部署服务

### 2.1 获取 coredns 的 docker 镜像

以下操作可以在任意节点上完成,推荐在 `7.200` 上做,因为接下来制作 coredns 的 k8s 配置清单也是在运维主机 `7.200` 上创建后,再到 node 节点上应用

```sh
docker pull docker.io/coredns/coredns:1.6.1

docker tag coredns/coredns:1.6.1 harbor.zq.com/public/coredns:v1.6.1

docker push harbor.zq.com/public/coredns:v1.6.1
```

### 2.2 创建 coredns 的资源配置清单

以下资源配置清单,都是参考官网改出来的

```sh
mkdir -p /data/k8s-yaml/coredns
```

#### 2.2.1 RBAC 集群权限清单

```sh
cat >/data/k8s-yaml/coredns/rbac.yaml <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: coredns
  namespace: kube-system
  labels:
      kubernetes.io/cluster-service: "true"
      addonmanager.kubernetes.io/mode: Reconcile
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
    addonmanager.kubernetes.io/mode: Reconcile
  name: system:coredns
rules:
- apiGroups:
  - ""
  resources:
  - endpoints
  - services
  - pods
  - namespaces
  verbs:
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
    addonmanager.kubernetes.io/mode: EnsureExists
  name: system:coredns
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:coredns
subjects:
- kind: ServiceAccount
  name: coredns
  namespace: kube-system
EOF
```

#### 2.2.2 configmap 配置清单

```sh
cat >/data/k8s-yaml/coredns/cm.yaml <<EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        log
        health
        ready
        kubernetes cluster.local 192.168.0.0/16  #service 资源 cluster 地址
        forward . 10.4.7.11   #上级 DNS 地址
        cache 30
        loop
        reload
        loadbalance
       }
EOF
```

#### 2.2.3 depoly 控制器清单

```sh
cat >/data/k8s-yaml/coredns/dp.yaml <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: coredns
  namespace: kube-system
  labels:
    k8s-app: coredns
    kubernetes.io/name: "CoreDNS"
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: coredns
  template:
    metadata:
      labels:
        k8s-app: coredns
    spec:
      priorityClassName: system-cluster-critical
      serviceAccountName: coredns
      containers:
      - name: coredns
        image: harbor.zq.com/public/coredns:v1.6.1
        args:
        - -conf
        - /etc/coredns/Corefile
        volumeMounts:
        - name: config-volume
          mountPath: /etc/coredns
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        - containerPort: 9153
          name: metrics
          protocol: TCP
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
      dnsPolicy: Default
      volumes:
        - name: config-volume
          configMap:
            name: coredns
            items:
            - key: Corefile
              path: Corefile
EOF
```

### 2.2.4 service 资源清单

```sh
cat >/data/k8s-yaml/coredns/svc.yaml <<EOF
apiVersion: v1
kind: Service
metadata:
  name: coredns
  namespace: kube-system
  labels:
    k8s-app: coredns
    kubernetes.io/cluster-service: "true"
    kubernetes.io/name: "CoreDNS"
spec:
  selector:
    k8s-app: coredns
  clusterIP: 192.168.0.2
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
  - name: metrics
    port: 9153
    protocol: TCP
EOF
```

### 2.3 创建资源并验证

在任意 Node 节点执行配置都可以,先

#### 2.3.1 验证服务能够访问`

```sh
[root@hdss7-21 ~]# dig -t A harbor.zq.com  +short
10.4.7.200
```

#### 2.3.2 创建资源:

```sh
kubectl create -f http://k8s-yaml.zq.com/coredns/rbac.yaml
kubectl create -f http://k8s-yaml.zq.com/coredns/cm.yaml
kubectl create -f http://k8s-yaml.zq.com/coredns/dp.yaml
kubectl create -f http://k8s-yaml.zq.com/coredns/svc.yaml
```

![mark](/images/20220417-k8s04-coredns-01.png)

#### 2.3.3. 查看创建情况:

```sh
kubectl get all -n kube-system
kubectl get svc -o wide -n kube-system
dig -t A www.baidu.com @192.168.0.2 +short
```

![mark](/images/20220417-k8s04-coredns-02.png)

#### 2.3.4 使用 dig 测试解析

```sh
[root@hdss7-21 ~]# dig -t A www.baidu.com @192.168.0.2 +short
www.a.shifen.com.
39.156.66.18
39.156.66.14
[root@hdss7-21 ~]# dig -t A harbor.zq.com @192.168.0.2 +short
10.4.7.200
```

coredns 已经能解析外网域名了,因为 coredns 的配置中,写了他的上级 DNS 为 `10.4.7.11`,如果它自己解析不出来域名,会通过递归查询一级级查找
但 coredns 我们不是用来做外网解析的,而是用来做 service 名和 serviceIP 的解析

#### 2.3.5 创建一个 service 资源来验证

先查看 `kube-public` 名称空间有没有 Pod

```sh
~]#  kubectl get pod  -n kube-public
No resources found.
# 之前我调试问题已经清理了所有的 Pod,所以没有
```

如果没有则先创建 Pod

```sh
kubectl create deployment nginx-dp --image=harbor.zq.com/public/nginx:v1.17.9 -n kube-public

~]# kubectl get deployments -n kube-public 
NAME       READY   UP-TO-DATE   AVAILABLE   AGE
nginx-dp   1/1     1            1           35s

~]#  kubectl get pod  -n kube-public
NAME                       READY   STATUS    RESTARTS   AGE
nginx-dp-568f8dc55-rxvx2   1/1     Running   0          56s
```

给 Pod 创建一个 service

```sh
kubectl expose deployment nginx-dp --port=80 -n kube-public

~]# kubectl -n kube-public get service
NAME       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
nginx-dp   ClusterIP   192.168.63.255   <none>        80/TCP    11s
```

验证是否可以解析

```sh
~]# dig -t A nginx-dp @192.168.0.2 +short

# 发现无返回数据,难道解析不了
# 其实是需要完整域名:服务名.名称空间.svc.cluster.local.

~]# dig -t A nginx-dp.kube-public.svc.cluster.local. @192.168.0.2 +short
192.168.63.255
```

> 可以看到我们没有手动添加任何解析记录，我们 nginx-dp 的 service 资源的 IP，已经被解析了：

进入到 Pod 内部再次验证

```sh
~]# kubectl -n kube-public exec -it nginx-dp-568f8dc55-rxvx2 /bin/bash
-qjwmz:/# apt update && apt install curl
-qjwmz:/# ping nginx-dp
PING nginx-dp.kube-public.svc.cluster.local (192.168.191.232): 56 data bytes
64 bytes from 192.168.191.232: icmp_seq=0 ttl=64 time=0.184 ms
64 bytes from 192.168.191.232: icmp_seq=1 ttl=64 time=0.225 ms
```

为什么在容器中不用加全域名?

```sh
-qjwmz:/# cat /etc/resolv.conf 
nameserver 192.168.0.2
search kube-public.svc.cluster.local svc.cluster.local cluster.local host.com
options ndots:5
```

> 当我进入到 Pod 内部以后，会发现我们的 dns 地址是我们的 coredns 地址，
> 
> 以及搜索域中已经添加了搜索域: `kube-public.svc.cluster.local`

我们解决了在集群内部解析的问题，要想在集群外部访问我们的服务还需要 `Ingress` 服务暴露功能

现在，我们已经解决了在集群内部解析的问题，但是我们怎么做到在集群外部访问我们的服务呢？

> 原文链接：<https://www.cnblogs.com/noah-luo/p/13345194.html>
