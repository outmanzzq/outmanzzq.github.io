---
layout: post
title: K8S(16) 集成实战-使用 Spinnaker 进行自动化部署
categories: k8s,devops
description: Spinnaker 是一个开源的多云持续交付平台，提供快速、可靠、稳定的软件变更服务。主要包含两类功能：集群管理和部署管理
keywords: k8s,spinnake,devops
---

> Spinnaker 是 Google 与 Netflix 发布的企业级持续交付平台，具有多云部署、自动发布、权限控制以及应用最佳实践等诸多优点。

- 官网：<https://spinnaker.io/>
- 文档: <https://spinnaker.io/docs/>


# K8S 集成实战-使用 Spinnaker 进行自动化部署

## 1 Spinnaker 概述和选型

### 1.1 概述

#### 1.1.1 主要功能

Spinnaker 是一个开源的多云持续交付平台，提供快速、可靠、稳定的软件变更服务。主要包含两类功能：集群管理和部署管理

#### 1.1.2 集群管理

集群管理主要用于管理云资源，Spinnaker 所说的”云“可以理解成 AWS，即主要是 LAAS 的资源，比如 OpenStak，Google 云，微软云等，后来还支持了容器与 Kubernetes，但是管理方式还是按照管理基础设施的模式来设计的。

#### 1.1.3 部署管理

管理部署流程是 Spinnaker 的核心功能，使用 Minio 作为持久化层，同时对接 Jenkins 流水线创建的镜像，部署到 Kubernetes 集群中去，让服务真正运行起来。

#### 1.1.4 逻辑架构图

Spinnaker 自己就是 Spinnake 一个微服务,由若干组件组成，整套逻辑架构图如下：

![mark](/images/20220512-k8s16-spinnake-01.png)

- **Deck** 是基于浏览器的 UI。
- **Gate** 是 API 网关。
  **Spinnaker** UI和所有api调用程序都通过 Gate 与 Spinnaker 进行通信。
- **Clouddriver** 负责管理云平台，并为所有部署的资源编制索引/缓存。
- **Front**50 用于管理数据持久化，用于保存应用程序，管道，项目和通知的元数据。
- **Igor** 用于通过 Jenkins 和 Travis CI 等系统中的持续集成作业来触发管道，并且它允许在管道中使用 Jenkins / Travis 阶段。
- **Orca** 是编排引擎。它处理所有临时操作和流水线。
- **Rosco** 是管理调度虚拟机。
- **Kayenta** 为 Spinnaker 提供自动化的金丝雀分析。
- **Fiat** 是 Spinnaker 的认证服务。
- **Echo** 是信息通信服务。它支持发送通知（例如，Slack，电子邮件，SMS），并处理来自 Github 之类的服务中传入的 Webhook。

### 1.2 部署选型

- Spinnaker官网: <https://www.spinnaker.io/>

  Spinnaker 包含组件众多,部署相对复杂,因此官方提供的脚手架工具 Halyard,但是可惜里面涉及的部分镜像地址被墙

- Armory发行版: <https://www.armory.io/>

  基于 Spinnaker,众多公司开发了开发第三方发行版来简化 Spinnaker 的部署工作,例如我们要用的 Armory 发行版

> Armory 也有自己的脚手架工具,虽然相对 Halyard 更简化了,但仍然部分被墙！ 因此我们部署的方式是手动交付 Spinnaker 的 Armory 发行版

## 2 部署 Spinnaker 第一部分

### 2.1 Spinnaker 之 Minio 部署

#### 2.1.1 准备 Minio 镜像

```sh
docker pull minio/minio:RELEASE.2019-12-30T05-45-39Z
docker tag c172312dda81 harbor.zq.com/armory/minio:latest
docker push harbor.zq.com/armory/minio:latest
```

准备目录

```sh
mkdir -p /data/nfs-volume/minio
mkdir -p /data/k8s-yaml/armory/minio
cd /data/k8s-yaml/armory/minio
```

#### 2.1.2 准备 Deployment 资源清单

```yaml
cat >dp.yaml <<'EOF'
kind: Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    name: minio
  name: minio
  namespace: armory
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 7
  selector:
    matchLabels:
      name: minio
  template:
    metadata:
      labels:
        app: minio
        name: minio
    spec:
      containers:
      - name: minio
        image: harbor.zq.com/armory/minio:latest
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 9000
          protocol: TCP
        args:
        - server
        - /data
        env:
        - name: MINIO_ACCESS_KEY
          value: admin
        - name: MINIO_SECRET_KEY
          value: admin123
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /minio/health/ready
            port: 9000
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        volumeMounts:
        - mountPath: /data
          name: data
      imagePullSecrets:
      - name: harbor
      volumes:
      - nfs:
          server: hdss7-200
          path: /data/nfs-volume/minio
        name: data 
EOF
```

#### 2.1.3 准备 SVC 资源清单

```yaml
cat >svc.yaml <<'EOF'
apiVersion: v1
kind: Service
metadata:
  name: minio
  namespace: armory
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 9000
  selector:
    app: minio
EOF
```

#### 2.1.4 准备 Ingress 资源清单

```yaml
cat >ingress.yaml <<'EOF' 
kind: Ingress
apiVersion: extensions/v1beta1
metadata:
  name: minio
  namespace: armory
spec:
  rules:
  - host: minio.zq.com
    http:
      paths:
      - path: /
        backend:
          serviceName: minio
          servicePort: 80
EOF
```

#### 2.1.5 应用资源配置清单

**任意 Node 节点**
创建namespace和secret

```sh
kubectl create namespace armory
kubectl create secret docker-registry harbor \
    --docker-server=harbor.zq.com \
    --docker-username=admin \
    --docker-password=Harbor12345 \
    -n armory
```

应用清单

```sh
kubectl apply -f http://k8s-yaml.zq.com/armory/minio/dp.yaml
kubectl apply -f http://k8s-yaml.zq.com/armory/minio/svc.yaml 
kubectl apply -f http://k8s-yaml.zq.com/armory/minio/ingress.yaml 
```

#### 2.1.6 访问验证

访问 <http://minio.zq.com> ,用户名密码为: minioadmin/minioadmin

如果访问并登陆成功,表示 Minio 部署成功

### 2.2 Spinnaker 之 Redis 部署

#### 2.2.1 准备镜像好目录

```armasm
docker pull redis:4.0.14
docker tag redis:4.0.14 harbor.zq.com/armory/redis:v4.0.14
docker push harbor.zq.com/armory/redis:v4.0.14
```

准备目录

```sh
mkdir -p /data/k8s-yaml/armory/redis
cd /data/k8s-yaml/armory/redis
```

#### 2.2.2 准备 Deployment 资源清单

```YAML
cat >dp.yaml <<'EOF'
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    name: redis
  name: redis
  namespace: armory
spec:
  replicas: 1
  revisionHistoryLimit: 7
  selector:
    matchLabels:
      name: redis
  template:
    metadata:
      labels:
        app: redis
        name: redis
    spec:
      containers:
      - name: redis
        image: harbor.zq.com/armory/redis:v4.0.14
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 6379
          protocol: TCP
      imagePullSecrets:
      - name: harbor
EOF
```

#### 2.2.3 准备 SVC 资源清单

```yaml
cat >svc.yaml <<'EOF'
apiVersion: v1
kind: Service
metadata:
  name: redis
  namespace: armory
spec:
  ports:
  - port: 6379
    protocol: TCP
    targetPort: 6379
  selector:
    app: redis
EOF
```

#### 2.3.4 应用资源配置清单

```sh
kubectl apply -f http://k8s-yaml.zq.com/armory/redis/dp.yaml
kubectl apply -f http://k8s-yaml.zq.com/armory/redis/svc.yaml
```

## 3 部署 Spinnaker 之 CloudDriver

CloudDriver 是整套 Spinnaker 部署中最难的部分,因此单独写一章来说明

### 3.1 部署准备工作

#### 3.1.1 准备镜像和目录

```sh
docker pull armory/spinnaker-clouddriver-slim:release-1.11.x-bee52673a
docker tag f1d52d01e28d harbor.od.com/armory/clouddriver:v1.11.x
docker push harbor.od.com/armory/clouddriver:v1.11.x
```

准备目录

```sh
mkdir /data/k8s-yaml/armory/clouddriver
cd /data/k8s-yaml/armory/clouddriver
```

#### 3.1.2 准备 Minio 的 Secret

准备配置文件

```sh
cat >credentials <<'EOF'
[default]
aws_access_key_id=admin
aws_secret_access_key=admin123
EOF
```

Node 节点创建 Secret

```sh
wget http://k8s-yaml.od.com/armory/clouddriver/credentials
kubectl create secret generic credentials \
    --from-file=./credentials \
    -n armory

# 也可以不急于配置文件,直接命令行创建
kubectl create secret generic credentials \
    --aws_access_key_id=admin \
    --aws_secret_access_key=admin123 \
    -n armory
```

#### 3.1.3 签发证书与私钥

```sh
cd /opt/certs
cp client-csr.json admin-csr.json
sed -i 's##cluster-admin#g' admin-csr.json
cfssl gencert \
    -ca=ca.pem \
    -ca-key=ca-key.pem \
    -config=ca-config.json \
    -profile=client \
    admin-csr.json |cfssl-json -bare admin
ls admin*
```

#### 3.1.3 分发证书

在任意 Node 节点

```sh
cd /opt/certs
scp hdss7-200:/opt/certs/ca.pem .
scp hdss7-200:/opt/certs/admin.pem .
scp hdss7-200:/opt/certs/admin-key.pem .
```

#### 3.1.4 创建用户

```sh
# 4步法创建用户
kubectl config set-cluster myk8s \
    --certificate-authority=./ca.pem \
    --embed-certs=true --server=https://10.4.7.10:7443 \
    --kubeconfig=config

kubectl config set-credentials cluster-admin \
    --client-certificate=./admin.pem \
    --client-key=./admin-key.pem \
    --embed-certs=true --kubeconfig=config

kubectl config set-context myk8s-context \
    --cluster=myk8s \
    --user=cluster-admin \
    --kubeconfig=config

kubectl config use-context myk8s-context \
    --kubeconfig=config
    
# 集群角色绑定
kubectl create clusterrolebinding myk8s-admin \
    --clusterrole=cluster-admin \
    --user=cluster-admin
```



## 附：在非 K8S 集群外管理 K8S

1. 将管理集群主机 /root/.kube/config  拷贝到主机 /root/.kube/ 目录下

   ```
   scp -r hdss7-21:/root/config /root/.kube/config
   ```

   

2. 将 /usr/bin/kubectl 客户端工具拷贝到目标主机 /usr/bin/kubectl  目录

   ```
   scp -r hdss7-21:/usr/bin/kubectl /usr/bin/kubectl
   ```

   

3. 写入目标主机 BASH 环境变量

   ```
   [root@hdss7-200 .kube]# echo "export KUBECONFIG=/root/.kube/config" >>/etc/profile
   
   [root@hdss7-200 .kube]# source /etc/profile
   ```

4. 验证结果

   ```
   [root@hdss7-200 .kube]# kubectl get svc -n infra
   NAME                   TYPE        CLUSTER-IP        EXTERNAL-IP   PORT(S)    AGE
   alertmanager           ClusterIP   192.168.126.150   <none>        80/TCP     15d
   apollo-configservice   ClusterIP   192.168.202.180   <none>        8080/TCP   19d
   apollo-portal          ClusterIP   192.168.49.230    <none>        8080/TCP   18d
   dubbo-monitor          ClusterIP   192.168.147.144   <none>        8080/TCP   21d
   grafana                ClusterIP   192.168.99.103    <none>        3000/TCP   15d
   jenkins                ClusterIP   192.168.102.162   <none>        80/TCP     22d
   kafka-manager          ClusterIP   192.168.180.23    <none>        9000/TCP   14d
   kibana                 ClusterIP   192.168.68.1      <none>        5601/TCP   12d
   prometheus             ClusterIP   192.168.53.13     <none>        9090/TCP   16d
   ```

   